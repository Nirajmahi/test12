{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\CCC-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\CCC-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CCC-PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file1 = r'C:\\Users\\CCC-PC\\Desktop\\Amazon Book\\review_dataset.csv'\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(file1, on_bad_lines='skip', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>review_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>the title says it all ive loved the authors ot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>the book has good principles but does not help...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>i dont understand why buy this version by jane...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>i am not a student and wanted to read a somewh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>this is a wonderfully written book i have lear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                         reviewText  review_label\n",
       "0      2.0  the title says it all ive loved the authors ot...             0\n",
       "1      2.0  the book has good principles but does not help...             0\n",
       "2      1.0  i dont understand why buy this version by jane...             0\n",
       "3      2.0  i am not a student and wanted to read a somewh...             0\n",
       "4      1.0  this is a wonderfully written book i have lear...             0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming df1['reviewText'] is your column of text data\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize the text into words\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    # Reconstruct the text\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# Apply the function to remove stopwords from each review\n",
    "df1['reviewText'] = df1['reviewText'].apply(lambda text: remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_alphanumeric_tokens(text):\n",
    "    cleaned_text = re.sub(r'\\b\\w*\\d+\\w*\\b', '', text)\n",
    "    return re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "df1['reviewText'] = df1['reviewText'].apply(remove_alphanumeric_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df1['reviewText'])\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uuuuffffff': 312781, 'jackandpam': 312782, 'trijicon': 312783, 'mro': 312784, 'expressway': 312785, 'gpses': 312786, 'imeadiatly': 312787, \"'roomy\": 312788, 'caplets': 312789, 'wildland': 312790, 'topomaps': 312791, 'maping': 312792, 'danbury': 312793}\n",
      "312794\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(word_index.items())[312780:312794]))\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence to word split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(df1['reviewText'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299990"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Padding sequences to the same length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2900\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_target_pairs(sequences, window_size):\n",
    "   \n",
    "    for seq in sequences:\n",
    "        sequence_length = len(seq)\n",
    "        for i, word_id in enumerate(seq):\n",
    "            # Calculate window range\n",
    "            start = max(i - window_size, 0)\n",
    "            end = min(i + window_size + 1, sequence_length)\n",
    "            \n",
    "            # Prepare context words\n",
    "            context = [seq[j] for j in range(start, end) if j != i]\n",
    "            \n",
    "            # Yield the current context and target word\n",
    "            yield context, word_id\n",
    "\n",
    "# Example of using the generator to print first few context-target pairs\n",
    "for context, target in generate_context_target_pairs(sequences, window_size=3):\n",
    "    print(f\"Context: {context}, Target: {target}\")\n",
    "      # Remove or adjust this break statement as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "targets = []\n",
    "\n",
    "for context, target in generate_context_target_pairs(sequences, window_size):\n",
    "    contexts.append(context)\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(c) for c in contexts)  # Determine the maximum length\n",
    "contexts_padded = pad_sequences(contexts, maxlen=max_length, padding='post', truncating='post')\n",
    "targets = np.array(targets)\n",
    "X_train, X_test, y_train, y_test = train_test_split(contexts_padded, targets, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   12,    52,   582, ...,  5277, 22612,   797])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19985219,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19985219, 6)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=100\n",
    "window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = tf.Variable(tf.random.uniform([vocab_size, embedding_dim], -1.0, 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corruption(x, p=0.5):\n",
    "    random_tensor = 1 - p\n",
    "    random_tensor += tf.random.uniform(tf.shape(x))\n",
    "    dropout_mask = tf.floor(random_tensor)\n",
    "    return tf.divide(x, 1 - p) * dropout_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDSAWEModel(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CDSAWEModel, self).__init__()\n",
    "        self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, embeddings_initializer='uniform')\n",
    "        self.dense_sentiment = Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs: list of two items, [context_words, target_word]\n",
    "        context_words, target_word = inputs\n",
    "        context_embeddings = self.embedding(context_words)\n",
    "        \n",
    "        # Average the context embeddings as per Equation (2)\n",
    "        avg_context_embedding = tf.reduce_mean(context_embeddings, axis=1)\n",
    "        \n",
    "        # Corrupt the context embedding\n",
    "        corrupted_context_embedding = corruption(avg_context_embedding)\n",
    "        \n",
    "        # Prediction from corrupted context\n",
    "        sentiment_prediction = self.dense_sentiment(corrupted_context_embedding)\n",
    "        \n",
    "        return corrupted_context_embedding, sentiment_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\CCC-PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CDSAWEModel(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_words = tf.keras.Input(shape=(window_size,), dtype=tf.int32)\n",
    "target_word = tf.keras.Input(shape=(), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_context_embedding, sentiment_prediction = model([context_words, target_word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
