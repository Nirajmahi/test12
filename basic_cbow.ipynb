{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bafd780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\niraj\n",
      "[nltk_data]     mahi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a736e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e825c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file1 = r'final_review_dataset.csv'\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(file1, on_bad_lines='skip', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fbcf4f",
   "metadata": {},
   "source": [
    "From now the  modification of context,center and sentiment label is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "44252bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(words, C):\n",
    "    \n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i + 1):(i + C + 1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "C=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0142baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(data_point_index, vocabsize):\n",
    "    temp = np.zeros(vocabsize)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0fe10373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file1 = r'final_review_dataset.csv'\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(file1, on_bad_lines='skip', low_memory=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2556df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4737888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "63edb791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     box says works   feet away   nt tested yetbut ...\n",
       "1                                 product registeration\n",
       "2     overall wierd movie   saw description guide sa...\n",
       "3                                            piece junk\n",
       "4     mark hamon character twist   turns worth purchase\n",
       "                            ...                        \n",
       "95                                           ugly weird\n",
       "96                 router good wireless card nt connect\n",
       "97                                        great cameras\n",
       "98    finally clint eastwood movie belong dirty harr...\n",
       "99                                best car receiver far\n",
       "Name: reviewText, Length: 100, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "83f688b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in wordList: 3249\n",
      "Total number of unique words in vocabulary: 1952\n"
     ]
    }
   ],
   "source": [
    "wordList = []  \n",
    "vocabulary = set()  \n",
    "\n",
    "for review_text in df1['reviewText']:\n",
    "    words = review_text.split()  \n",
    "    wordList.extend(words)  # Add all words to wordList\n",
    "    vocabulary.update(words)  \n",
    "\n",
    "print(\"Total number of words in wordList:\", len(wordList))\n",
    "print(\"Total number of unique words in vocabulary:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a3487968",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabsize= len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c0b0a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_int={}\n",
    "int_2_word={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27c6837a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in wordList: 1952\n",
      "Number of words in word2int: 1952\n",
      "Number of indices in int2word: 1952\n"
     ]
    }
   ],
   "source": [
    "wordList = list(vocabulary)  # Assuming vocabulary contains the unique words\n",
    "print(\"Number of words in wordList:\", len(wordList))\n",
    "word_2_int={}\n",
    "int_2_word={}\n",
    "for i, word in enumerate(wordList):\n",
    "    word_2_int[word] = i  \n",
    "    int_2_word[i] = word\n",
    "\n",
    "print(\"Number of words in word2int:\", len(word_2_int))\n",
    "print(\"Number of indices in int2word:\", len(int_2_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4d766b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3052\n",
      "3052\n",
      "100\n",
      "['box', 'works']\n"
     ]
    }
   ],
   "source": [
    "context_data = []\n",
    "senti_data = []\n",
    "center_data=[]\n",
    "i = 0\n",
    "for index, row in df1.iterrows():\n",
    "    \n",
    "  \n",
    "    words = row['reviewText'].split()\n",
    "    sentiment_label = row['review_label']\n",
    "   \n",
    "\n",
    "    for context_words, center_word  in get_windows(words, C):\n",
    "        context_data.append(context_words)\n",
    "        senti_data.append(sentiment_label)\n",
    "        center_data.append(center_word)\n",
    "    i=i+1\n",
    "#print(context_data)\n",
    "#print(senti_data)\n",
    "print(len(context_data))\n",
    "print(len(senti_data))\n",
    "print(i)\n",
    "print(context_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df68f6a1",
   "metadata": {},
   "source": [
    "### Context Target Word  one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b55c0f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "context_data_hot = []\n",
    "\n",
    "center_data_hot=[]\n",
    "\n",
    "i=0\n",
    "\n",
    "for context_word,center_word in zip(context_data,center_data):\n",
    "    \n",
    "    \n",
    "    \n",
    "    context_words_vectors = [to_one_hot(word_2_int[w], vocabsize) for w in context_word]\n",
    "    context_data_hot.append(np.mean(context_words_vectors,axis=0))\n",
    "    center_data_hot.append(to_one_hot(word_2_int[center_word], vocabsize))\n",
    "    \n",
    "    i=i+1\n",
    "print(len( context_words_vectors ))\n",
    "print(context_data_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5616c850",
   "metadata": {},
   "source": [
    "## Batch Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6365b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(batch_size):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    batch_senti=[]\n",
    "    for x, y,z in zip ( context_data_hot,center_data_hot,senti_data):\n",
    "        while len(batch_x) < batch_size:\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "            batch_senti.append(z)\n",
    "        else:\n",
    "            yield np.array(batch_x).T, np.array(batch_y).T,np.array(batch_senti).T\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d085b40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1952, 7)\n",
      "(1952, 7)\n",
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "\n",
    "for x,y, z in get_batches(7):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(z.shape)\n",
    "    i=i+1\n",
    "    if i==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "163684f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['reviewText'] = df1['reviewText'].apply(lambda x: re.sub(r'[,!?;-]', '.', x))\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "data = nltk.word_tokenize(df1['reviewText'].str.cat(sep=' '))\n",
    "data = [ch.lower() for ch in data if ch.isalpha() or ch == '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "381a3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "   \n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "\n",
    "def get_idx(words, word2Ind):\n",
    "    idx = []\n",
    "    for word in words:\n",
    "        idx = idx + [word2Ind[word]]\n",
    "    return idx\n",
    "\n",
    "\n",
    "def pack_idx_with_frequency(context_words, word2Ind):\n",
    "    freq_dict = defaultdict(int)\n",
    "    for word in context_words:\n",
    "        freq_dict[word] += 1\n",
    "    idxs = get_idx(context_words, word2Ind)\n",
    "    packed = []\n",
    "    for i in range(len(idxs)):\n",
    "        idx = idxs[i]\n",
    "        freq = freq_dict[context_words[i]]\n",
    "        packed.append((idx, freq))\n",
    "    print(\"packed\")\n",
    "    print(packed)\n",
    "    return packed\n",
    "\n",
    "\n",
    "def get_vectors(data, word2Ind, V, C):\n",
    "    i = C\n",
    "    while True:\n",
    "        y = np.zeros(V)\n",
    "        x = np.zeros(V)\n",
    "        center_word = data[i]\n",
    "        y[word2Ind[center_word]] = 1\n",
    "        context_words = data[(i - C):i] + data[(i+1):(i+C+1)]\n",
    "        print(\"context_word\")\n",
    "        print(context_words)\n",
    "        num_ctx_words = len(context_words)\n",
    "        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):\n",
    "            x[idx] = freq/num_ctx_words\n",
    "        yield x, y\n",
    "        i += 1\n",
    "        if i >= len(data):\n",
    "            print('i is being set to 0')\n",
    "            i = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_pca(data, n_components=2):\n",
    "   \n",
    "    m, n = data.shape\n",
    "\n",
    "   \n",
    "    data -= data.mean(axis=0)\n",
    "  \n",
    "    R = np.cov(data, rowvar=False)\n",
    "   \n",
    "    evals, evecs = linalg.eigh(R)\n",
    "   \n",
    "    idx = np.argsort(evals)[::-1]\n",
    "\n",
    "    evecs = evecs[:, idx]\n",
    "   \n",
    "    evals = evals[idx]\n",
    " \n",
    "    evecs = evecs[:, :n_components]\n",
    "   \n",
    "    return np.dot(evecs.T, data.T).T\n",
    "\n",
    "\n",
    "def get_dict(data):\n",
    "  \n",
    "    words = sorted(list(set(data)))\n",
    "    n = len(words)\n",
    "    idx = 0\n",
    "  \n",
    "    word2Ind = {}\n",
    "    Ind2word = {}\n",
    "    for k in words:\n",
    "        word2Ind[k] = idx\n",
    "        Ind2word[idx] = k\n",
    "        idx += 1\n",
    "    return word2Ind, Ind2word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e8bf4",
   "metadata": {},
   "source": [
    "## From there  this will modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cd322db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V,sentiment_classes, random_seed=1):\n",
    "   \n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "   \n",
    "    \n",
    "    W1 = np.random.rand(N,V)\n",
    "   \n",
    "    W2 = np.random.rand(V,N)\n",
    "   \n",
    "    b1 = np.random.rand(N,1)\n",
    "   \n",
    "    b2 = np.random.rand(V,1)\n",
    "\n",
    "    # Sentiment model parameters (logistic regression)\n",
    "    W_s = np.random.rand(1, N)  # Assuming binary classification for simplicity\n",
    "    b_s = np.random.rand(1, 1)\n",
    "   \n",
    "\n",
    "    return W1, W2, b1, b2,W_s,b_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f2599eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "   \n",
    "    e_z = np.exp(z)\n",
    "    yhat = e_z/np.sum(e_z,axis=0)\n",
    "  \n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "22e2db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2,W_sentiment,b_sentiment):\n",
    "   \n",
    "    \n",
    "   \n",
    "    h = np.dot(W1,x)+b1\n",
    "    \n",
    "   \n",
    "    h = np.maximum(0,h)\n",
    "    \n",
    "   \n",
    "    z = np.dot(W2,h)+b2\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    return z, h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "97ea70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  cross-entropy cost functioN\n",
    "def compute_cost(y, yhat, batch_size):\n",
    "    # cost function \n",
    "    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n",
    "    cost = - 1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c9415463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment_cost(y_sentiment, pred_s, batch_size):\n",
    "    # Binary cross-entropy cost for sentiment classification\n",
    "   epsilon = 1e-7  # A small constant\n",
    "   cost_s = -1 / batch_size * np.sum(y_sentiment * np.log(pred_s + epsilon) + (1 - y_sentiment) * np.log(1 - pred_s + epsilon))\n",
    "\n",
    "   cost_s = np.squeeze(cost_s)\n",
    "   return cost_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fa24969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, W_s,b_s, pred_s,y_sentiment, batch_size):\n",
    "    l1 = np.dot(W2.T, (yhat - y))\n",
    "    #  relu to l1\n",
    "    l1 = np.maximum(0, l1)\n",
    "    #  the gradient of W1\n",
    "    grad_W1 = np.dot(l1, x.T) / batch_size\n",
    "    #  the gradient of W2\n",
    "    grad_W2 = np.dot(yhat - y, h.T) / batch_size\n",
    "    #  the gradient of b1\n",
    "    grad_b1 = np.sum(l1, axis=1, keepdims=True) / batch_size\n",
    "    # the gradient of b2\n",
    "    grad_b2 = np.sum(yhat - y, axis=1, keepdims=True) / batch_size\n",
    "\n",
    "    ds = pred_s - y_sentiment\n",
    "    grad_W_s = np.dot(ds, h.T) / batch_size\n",
    "    grad_b_s = np.sum(ds, axis=1, keepdims=True) / batch_size\n",
    "   \n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2, grad_W_s, grad_b_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0a7bce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_prediction_model(W_s, b_s, h):\n",
    "    # Logistic regression for sentiment classification\n",
    "    z_s = np.dot(W_s, h) + b_s\n",
    "    pred_s = sigmoid(z_s)\n",
    "    print(\"pred_s\")\n",
    "    print(pred_s.shape)\n",
    "    return pred_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "48dbd036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent1( N, V, num_iters):\n",
    "\n",
    "   # W1, W2, b1, b2 ,W_s,b_s= initialize_model(N,V, random_seed=282)\n",
    "    batch_size = 10\n",
    "    iters = 0\n",
    "    C = 1\n",
    "  \n",
    "    for x, y ,z in get_batches(batch_size): \n",
    "       \n",
    "        print(\"y\")\n",
    "        print(type(y))\n",
    "        print(y)\n",
    "        print(y.shape)\n",
    "        print(\"x\")\n",
    "        print(type(x))\n",
    "        print(x)\n",
    "        print(x.shape)\n",
    "        print(\"z\")\n",
    "        print(type(z))\n",
    "        print(z.shape)\n",
    "\n",
    "        iters += 1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "       \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e3aa6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(N, V, num_iters, alpha=0.03,beta=0.05):\n",
    "    sentiment_classes = 2\n",
    "\n",
    "    W1, W2, b1, b2 ,W_s,b_s= initialize_model(N,V,1,random_seed = 282)\n",
    "    batch_size = N\n",
    "    iters = 0\n",
    "    C = 1\n",
    "  \n",
    "    for x, y ,y_sentiment in get_batches(batch_size): \n",
    "        \n",
    "        y_sentiment = y_sentiment.reshape(1, -1)\n",
    "      \n",
    "        z, h  = forward_prop(x, W1, W2, b1, b2,W_s,b_s)\n",
    "        pred_s= sentiment_prediction_model(W_s, b_s, h)\n",
    "        \n",
    "\n",
    "        \n",
    "        yhat = softmax(z)\n",
    "        \n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        sentiment_loss = compute_sentiment_cost(y_sentiment,pred_s, batch_size)  # y_sentiment are the true sentiment labels\n",
    "        total_loss = beta * cost + (1 - beta) * sentiment_loss\n",
    "\n",
    "        print(cost)\n",
    "        if ( (iters+1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "        \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2,grad_W_s, grad_b_s = back_prop(x, yhat, y, h, W1, W2, b1, b2, W_s,b_s, pred_s,y_sentiment, batch_size)\n",
    "        \n",
    "        \n",
    "        W1 -= alpha*grad_W1 \n",
    "        W2 -= alpha*grad_W2\n",
    "        b1 -= alpha*grad_b1\n",
    "        b2 -= alpha*grad_b2\n",
    "\n",
    "     # Update sentiment model parameters\n",
    "        W_s -= alpha * grad_W_s\n",
    "        b_s -= alpha * grad_b_s\n",
    "        \n",
    "        \n",
    "        \n",
    "        iters += 1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2319dc13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_s\n",
      "(1, 100)\n",
      "13.692964676981592\n",
      "pred_s\n",
      "(1, 100)\n",
      "9.029142731547003\n",
      "pred_s\n",
      "(1, 100)\n",
      "4.579901300712\n",
      "pred_s\n",
      "(1, 100)\n",
      "0.7622023533647546\n",
      "pred_s\n",
      "(1, 100)\n",
      "0.22175012613268041\n"
     ]
    }
   ],
   "source": [
    "C = 1\n",
    "N = 100\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "W1, W2, b1, b2 ,W_s,b_s= initialize_model(N,vocabsize, 2,random_seed = 282)\n",
    "\n",
    "V = len(word2Ind)\n",
    "num_iters = 5\n",
    "W1, W2, b1, b2 = gradient_descent( N, vocabsize, num_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "18bb29a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1952)\n"
     ]
    }
   ],
   "source": [
    "print(W1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ef9b9804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36842629 0.39172236 0.12117024 ... 0.86283221 0.80354769 0.42350652]\n",
      " [0.51111431 0.72332409 0.93370004 ... 0.90829277 0.98011031 0.86157251]\n",
      " [0.87825143 0.04246476 0.58769782 ... 0.4974234  0.49062177 0.39280315]\n",
      " ...\n",
      " [0.64592789 0.7343668  0.62083031 ... 0.73095623 0.51407979 0.0657923 ]\n",
      " [0.63312484 0.06421244 0.79844273 ... 0.14403548 0.01158569 0.4114133 ]\n",
      " [0.63868317 0.13050883 0.07287024 ... 0.7325165  0.36827626 0.98729761]]\n"
     ]
    }
   ],
   "source": [
    "print(W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab0e45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2385fece",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[191], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmud\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m embs \u001b[38;5;241m=\u001b[39m (W1\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m W2)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m idx \u001b[38;5;241m=\u001b[39m [word2Ind[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m embs[idx, :]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape, idx) \n",
      "Cell \u001b[1;32mIn[191], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmud\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m embs \u001b[38;5;241m=\u001b[39m (W1\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m W2)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m idx \u001b[38;5;241m=\u001b[39m [\u001b[43mword2Ind\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m embs[idx, :]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape, idx) \n",
      "\u001b[1;31mKeyError\u001b[0m: 'mud'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "words = ['mud']\n",
    "\n",
    "embs = (W1.T + W2)/2.0\n",
    " \n",
    "\n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx, :]\n",
    "print(X.shape, idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f73324",
   "metadata": {},
   "outputs": [],
   "source": [
    "result= compute_pca(X, 2)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "result= compute_pca(X, 4)\n",
    "pyplot.scatter(result[:, 3], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a01e6",
   "metadata": {},
   "source": [
    "## Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7565417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embeddings(domain):\n",
    "    # Load pre-trained word embeddings for domain\n",
    "    # This should return a dictionary of word vectors\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "84e45f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_frequency(domain):\n",
    "    # Load or calculate word frequencies for the domain corpus\n",
    "    # This should return a dictionary of word frequencies\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4b2a61c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d3f30475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain relevance function using Sørensen-Dice coefficient\n",
    "def domain_relevance(w, freq_P, freq_Q):\n",
    "    return 2 * freq_P[w] * freq_Q[w] / (freq_P[w] + freq_Q[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "98e334e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information transfer function\n",
    "def information_transfer(phi_w, lambda_):\n",
    "    return sigmoid(lambda_ * phi_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fd2db88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def compute_loss(W_p, W_q, L, freq_P, freq_Q, lambda_):\n",
    "    loss_Q = 0  # Initialize domain Q loss\n",
    "    for w in L:\n",
    "        phi_w = domain_relevance(w, freq_P, freq_Q)\n",
    "        t_w = information_transfer(phi_w, lambda_)\n",
    "        loss_Q += t_w * np.linalg.norm(W_p[w] - W_q[w])**2\n",
    "    return loss_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b15a20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "lambda_ = 0.1  # Hyperparameter for scaling the domain correlation function\n",
    "epochs = 10    # Number of epochs to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "430acb66",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[199], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m freq_Q \u001b[38;5;241m=\u001b[39m load_corpus_frequency(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Word frequencies in domain Q\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize domain Q embeddings\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m W_q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m*\u001b[39m\u001b[43mW_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create word list L of words that appear in both domains\u001b[39;00m\n\u001b[0;32m     10\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(freq_P\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(freq_Q\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "W_s = load_word_embeddings('P')  # Pre-trained embeddings from domain P\n",
    "freq_P = load_corpus_frequency('P')  # Word frequencies in domain P\n",
    "freq_Q = load_corpus_frequency('Q')  # Word frequencies in domain Q\n",
    "\n",
    "# Initialize domain Q embeddings\n",
    "W_q = np.random.rand(*W_s.shape)\n",
    "\n",
    "# Create word list L of words that appear in both domains\n",
    "L = list(set(freq_P.keys()) & set(freq_Q.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ff7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Initialize gradient accumulator\n",
    "    grad_accumulator = np.zeros_like(W_q)\n",
    "\n",
    "    # Compute the gradients for each word\n",
    "    for word in L:\n",
    "        idx = word_indices[word]\n",
    "        phi_w = domain_relevance(word, freq_P, freq_Q)\n",
    "        t_w = information_transfer(phi_w, lambda_)\n",
    "        \n",
    "        # Gradient of the loss with respect to the word vector\n",
    "        grad_w = 2 * t_w * (W_q[idx] - W_p_array[idx])\n",
    "        \n",
    "        # Accumulate the gradients\n",
    "        grad_accumulator[idx] += grad_w\n",
    "\n",
    "    # Update W_q with the accumulated gradients\n",
    "    W_q -= learning_rate * grad_accumulator\n",
    "\n",
    "    # Calculate and print the loss (optional)\n",
    "    loss_Q = compute_loss(W_p_array, W_q, L, freq_P, freq_Q, lambda_)\n",
    "    print(f'Epoch {epoch}, Loss: {loss_Q}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
